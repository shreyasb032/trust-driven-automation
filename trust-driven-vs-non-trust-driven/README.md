# The main idea here is to compare how trust evolves if a robot considers it in its decision-making vs if it does not

In the trust-driven case, recommendations are generated by modeling the trust dynamics and using them in the optimization routine

In the non-trust-driven case, optimal actions are generated by directly solving the MDP, without considering trust

It will be interesting to see how trust evolves when there are different degrees of value alignment between the human's and the robot's reward functions
